{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-fingerprint.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/connorj4/serial-multibiometric-face-fingerprint/blob/master/cnn_fingerprint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nXRYFcKeeCLh",
        "colab_type": "code",
        "outputId": "72acabfd-1729-41d7-831c-323f719e57ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "gdrive_home_dir = '/content/drive/My Drive/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g310W7Bcmqtp",
        "colab_type": "code",
        "outputId": "8c95e655-57cd-4099-8acc-a02e13bfca62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/dataset_fp/labelfinger.csv'\n",
        "!ls '/content/drive/My Drive/dataset_fp/data/'\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/dataset_fp/labelfinger.csv'\n",
            "10.jpg\t18.jpg\t25.jpg\t32.jpg\t3.jpg\t47.jpg\t54.jpg\t61.jpg\t69.jpg\t76.jpg\n",
            "11.jpg\t19.jpg\t26.jpg\t33.jpg\t40.jpg\t48.jpg\t55.jpg\t62.jpg\t6.jpg\t77.jpg\n",
            "12.jpg\t1.jpg\t27.jpg\t34.jpg\t41.jpg\t49.jpg\t56.jpg\t63.jpg\t70.jpg\t78.jpg\n",
            "13.jpg\t20.jpg\t28.jpg\t35.jpg\t42.jpg\t4.jpg\t57.jpg\t64.jpg\t71.jpg\t79.jpg\n",
            "14.jpg\t21.jpg\t29.jpg\t36.jpg\t43.jpg\t50.jpg\t58.jpg\t65.jpg\t72.jpg\t7.jpg\n",
            "15.jpg\t22.jpg\t2.jpg\t37.jpg\t44.jpg\t51.jpg\t59.jpg\t66.jpg\t73.jpg\t80.jpg\n",
            "16.jpg\t23.jpg\t30.jpg\t38.jpg\t45.jpg\t52.jpg\t5.jpg\t67.jpg\t74.jpg\t8.jpg\n",
            "17.jpg\t24.jpg\t31.jpg\t39.jpg\t46.jpg\t53.jpg\t60.jpg\t68.jpg\t75.jpg\t9.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tSHR-OA1c40N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sS3_oeBJcoWP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(1337)  # for reproducibility"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LyK02LtMcq6-",
        "colab_type": "code",
        "outputId": "13441e5f-a097-4522-b29c-f64ebfb3adb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0T8w8xsKcpM7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "nb_classes = 10\n",
        "nb_epoch = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uDpUl8gycjD9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To make the data ready for CNN, pictures are named with indexes, like '1.jpg', '2.jpg', etc..\n",
        "def dir_to_dataset(mypath, loc_train_labels=\"\"):\n",
        "    dataset = []\n",
        "    \n",
        "    gbr = pd.read_csv(loc_train_labels, sep=\"\\t\")\n",
        "\n",
        "    #for file_count, file_name in enumerate( sorted(glob(glob_files),key=len) ):\n",
        "    for i in range(1,81):\n",
        "        image = Image.open(mypath + str(i)+'.jpg')\n",
        "        img = Image.open(mypath + str(i)+'.jpg').convert('LA') #tograyscale\n",
        "        '''\n",
        "        pixels_list = list(img.getdata())\n",
        "        for pixel in pixels_list:\n",
        "          p = pixel\n",
        "          dataset.append(p)\n",
        "        '''\n",
        "        #pixels_list = list(img.getdata())\n",
        "        #width, height = img.size\n",
        "        #pixels = [pixels[i * width:(i + 1) * width] for i in range(height)]\n",
        "        \n",
        "        pixels = [f[0] for f in list(img.getdata())] #<- original\n",
        "        dataset.append(pixels)\n",
        "    # outfile = glob_files+\"out\"\n",
        "    # np.save(outfile, dataset)\n",
        "    if len(loc_train_labels) > 0:\n",
        "        df = pd.read_csv(loc_train_labels)\n",
        "        return np.array(dataset), gbr[\"Class\"].values\n",
        "    else:\n",
        "        return np.array(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ZglATaj7btE",
        "colab_type": "code",
        "outputId": "9cd4e7cf-afdf-4224-9f34-871a2f8583d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    Data, y = dir_to_dataset(\"/content/drive/My Drive/dataset_fp/data/\",\"/content/drive/My Drive/dataset_fp/labelfinger.csv\")\n",
        "    \n",
        "    #Split the train set and validation set\n",
        "    train_set_x = Data[:60]\n",
        "    val_set_x = Data[10:]\n",
        "    train_set_y = y[:60]\n",
        "    val_set_y = y[10:]\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = (train_set_x,train_set_y),(val_set_x,val_set_y)\n",
        "\n",
        "    # input image dimensions\n",
        "    img_rows, img_cols = 640, 480\n",
        "    # number of convolutional filters to use\n",
        "    nb_filters = 32\n",
        "    # size of pooling area for max pooling\n",
        "    pool_size = (2, 2)\n",
        "    # convolution kernel size\n",
        "    kernel_size = (3, 3)\n",
        "    \n",
        "    \n",
        "\n",
        "    # Checking if the backend is Theano or Tensorflow\n",
        "    if K.image_dim_ordering() == 'th':\n",
        "        X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "        X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "        input_shape = (1, img_rows, img_cols)\n",
        "    else:\n",
        "        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1) \n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "    print('X_train shape:', X_train.shape)\n",
        "    print(X_train.shape[0], 'train samples')\n",
        "    print(X_test.shape[0], 'test samples')\n",
        "\n",
        "    # convert class vectors to binary class matrices\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
        "                    border_mode='valid',\n",
        "                    input_shape=input_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(128))\n",
        "    model.add(Dense(32))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(nb_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "            optimizer='sgd',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
        "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    \n",
        "    print(model.summary())\n",
        "    \n",
        "    print('Test score:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "    \n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (60, 640, 480, 1)\n",
            "60 train samples\n",
            "70 test samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(640, 480,..., padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60 samples, validate on 70 samples\n",
            "Epoch 1/2\n",
            "60/60 [==============================] - 2s 38ms/step - loss: 4.7499 - acc: 0.0833 - val_loss: 2.3080 - val_acc: 0.1143\n",
            "Epoch 2/2\n",
            "60/60 [==============================] - 1s 19ms/step - loss: 2.3138 - acc: 0.0833 - val_loss: 2.2992 - val_acc: 0.1857\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 638, 478, 32)      320       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 638, 478, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 636, 476, 32)      9248      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 636, 476, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 318, 238, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 318, 238, 32)      0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2421888)           0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 32)                77500448  \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 77,510,346\n",
            "Trainable params: 77,510,346\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Test score: 2.2991795131138395\n",
            "Test accuracy: 0.18571428614003319\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}